{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ace58cd-e015-45d9-87e2-88aee0c393b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import codecs\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e89dc1-c474-4f94-a7db-ddf7e3cfd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_word_cut(mytext,wordFlag=\"n,v,a,x\"):\n",
    "     jieba.load_userdict('prepare/用户自定义词典.txt');#加入自定义语料库\n",
    "     words = pseg.cut(mytext)\n",
    "     line_List=[]\n",
    "     flag_List=wordFlag.split(\",\")\n",
    "     for w in words:\n",
    "         i=0;\n",
    "         for flag in flag_List:#加入自定义语料库\n",
    "            if(w.flag==flag_List[i] and not w.word.isnumeric() and w.word not in stopwordeu and len(re.findall('\\s',w.word))==0):\n",
    "                 line_List.append(w.word)\n",
    "            i=i+1\n",
    "     line=\"\"\n",
    "     for word in line_List:\n",
    "          line+=word+\" \"\n",
    "     return line\n",
    "def prepare_data(content_list,attr1,attrmess,stopwords):\n",
    " #将数据格式进行转换[[数值，内容],[数值，内容],[数值，内容],[数值，内容]]\n",
    "     df = pd.DataFrame(content_list, columns=[attr1,attrmess])\n",
    "     #将数据格式进行转换\n",
    "     df[\"內容切分\"] = df[attrmess].astype(str).apply(chinese_word_cut)\n",
    "     n_features = 3000000\n",
    "     tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "     max_features=n_features,\n",
    "     stop_words=stopwords,\n",
    "      max_df = 0.5,\n",
    "      min_df = 20)\n",
    "     tf = tf_vectorizer.fit_transform(df['內容切分'])\n",
    "     return df,tf, tf_vectorizer\n",
    "def print_top_words2(model, feature_names, n_top_words):\n",
    "     t=0;\n",
    "     for topic_idx, topic in enumerate(model.components_):\n",
    "         topicSum=np.sum(topic, axis=0)\n",
    "         topic=topic/topicSum\n",
    "         #topic 中存放的主题-词汇矩阵，格式是 ndarray\n",
    "         print(\"Topic #%d:\" % topic_idx)\n",
    "    #tw_list.append(\"{name:Topic\"+str(topic_idx)+\",\"+\"value:1,symbolSize:15\n",
    "    # ,category:Topic\"+str(topic_idx)+\",draggable:false}\");\n",
    "     #argsort 返回从原始数值中，值小到大的索引值\n",
    "         print(\" | \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "         for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "                 t=t+1\n",
    "         print(\" | \".join([str(topic[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "def load_stopword(path): \n",
    "     stopwords = open(path,'r',encoding='utf-8').read()\n",
    "     stopwords = stopwords.split('\\n')\n",
    "     return stopwords\n",
    "def train_one_step(n_components,tf,tf_vectorizer,n_top_words):\n",
    "     lda = LatentDirichletAllocation(n_components, max_iter=200,\n",
    "     learning_method='online',\n",
    "     learning_offset=50.,\n",
    "     random_state=0)\n",
    "     lda.fit(tf)\n",
    "     print(' n_components:%s'%n_components)\n",
    "     tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "     print_top_words2(lda, tf_feature_names, n_top_words)\n",
    "     doc_word = tf.toarray()\n",
    "     doc_topic = lda.transform(tf)\n",
    "     topic_word = lda.components_\n",
    "     #计算困惑度\n",
    "     print(u'困惑度：')\n",
    "     print(lda.perplexity(tf,sub_sampling = False))\n",
    "    # data = pyLDAvis.sklearn.prepare(lda,tf,tf_vectorizer)\n",
    "    # pyLDAvis.show(data)\n",
    "     return doc_word, doc_topic, topic_word\n",
    "import math\n",
    "def savetopic_wordMat(topic_word,savePath):\n",
    "     tw_list=topic_word.tolist()\n",
    "     fw = open(savePath,'w')\n",
    "     topicNum=0;\n",
    "     for tw_line in tw_list:\n",
    "         z=0;\n",
    "         lineSum=sum(tw_line); \n",
    "         shangzhi=0;\n",
    "         for j in tw_line:\n",
    "             wordProbability=j/lineSum;\n",
    "             if(z<len(tw_line)-1):\n",
    "                fw.write(str(wordProbability)+\" \")\n",
    "             else:\n",
    "                fw.write(str(wordProbability)+\"\\n\")\n",
    "             z=z+1\n",
    "             temp=wordProbability*math.log(wordProbability)\n",
    "             shangzhi=shangzhi+temp;\n",
    "         print(\"Topic:\"+str(topicNum)+\"的信息熵是：\"+str(-shangzhi))\n",
    "         topicNum=topicNum+1\n",
    "def getMaxTopicFlag(doc_topic):\n",
    "     new_dt=[]\n",
    "     [rows, cols] = doc_topic.shape \n",
    "     for i in range(rows):\n",
    "         maxflag=doc_topic[i].argsort()[-1]\n",
    "         listTemp=doc_topic[i].tolist()\n",
    "         listTemp.append(maxflag)\n",
    "         new_dt.append(listTemp) \n",
    "     dtend=np.array(new_dt)\n",
    "     return dtend\n",
    "def saveDocTopicMat(attr1,attrmess,savePath,dtendMatrix):\n",
    "     cols = ['Topic %s'%i for i in range(best_tops+1)]\n",
    "     doc_topic = pd.DataFrame(dtendMatrix, columns=cols)\n",
    "     new_df = pd.concat([df[[attr1,attrmess]], doc_topic], axis=1)\n",
    "     print(new_df)\n",
    "     new_df.to_excel(savePath, index=False)\n",
    "     print(len(new_df))\n",
    "def readCroups4FilePath(path):\n",
    "    end_list=[]\n",
    "    df=pd.read_csv(path)\n",
    "    replacetext=[ # \"本报[\\S]+电\",\n",
    "                 # \"（本报[\\S]+电）\",\n",
    "                 \"新华社[\\S]+日电\",\n",
    "                 \"人民网[\\S]+日电\",\n",
    "                 \"（(.*?)版权(.*?)）\",\n",
    "                 \"服务邮箱：[\\s\\S]+\",\n",
    "                 \"互联网新闻信息服务许可证(.*?)+\",\n",
    "                 \"分享让更多人看到(.*?)+\",\n",
    "                 \"《 人民日报 》（[\\s\\S]+\",\n",
    "                 \"学习路上 时习之[\\s\\S]+\",\n",
    "                 \"\\r\\n\\|\\r\\n[\\s\\S]+\",                                                                                                                                             \n",
    "                 \"（(.*?)记者(.*?)）\"]\n",
    "    t=0\n",
    "    for index,row in df.iterrows():\n",
    "        list1=[]\n",
    "        t+=1\n",
    "        for replace in replacetext:\n",
    "              row[4]=re.sub(replace,\"\",str(row[4]))\n",
    "        list1.append(t)\n",
    "        list1.append(row[4])\n",
    "        end_list.append(list1)\n",
    "    return end_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b67be6-27fc-4206-9af9-bf1eb296cf6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "     #文件名称\n",
    "     # fileName=\"law\"\n",
    "     attr1='标题'\n",
    "     attrmess='内容'\n",
    "     stopwordeu=load_stopword(\"prepare/stopwords.txt\") \n",
    "     #读取文件夹\n",
    "     end_list=readCroups4FilePath(\"data/中国能源网全部.csv\");\n",
    "     print(\"文件数：\" , len(end_list)) \n",
    "     #最优主题个数\n",
    "     #组织语料\n",
    "     df, tf, tf_vectorizer=prepare_data(end_list,attr1,attrmess,stopwordeu)\n",
    "     # #训练\n",
    "     best_tops=21\n",
    "     doc_word,doc_topic, topic_word=train_one_step(best_tops,tf,tf_vectorizer,n_top_words=20)\n",
    "     dtendMatrix=getMaxTopicFlag(doc_topic)\n",
    "     savetopic_wordMat(topic_word,'result/'+'doc_topic'+str(best_tops)+'temptw.txt')\n",
    "     saveDocTopicMat(attr1,attrmess,'result/'+'doc_topic'+str(best_tops)+'tempdt.xlsx',dtendMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34a05b-94e7-419d-acd2-a0dabfc58b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
